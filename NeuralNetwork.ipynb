{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "712b3d41-a655-4979-85f3-61a6e4b9821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NeuralNetworkModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NeuralNetworkModel.py\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.parameters = {}\n",
    "        self.cache = []\n",
    "        self.grads = {}\n",
    "        self.params = {}\n",
    "        self.mini_batches = []\n",
    "    \n",
    "    def add(self,units,activation):\n",
    "        self.layers.append([units,activation])\n",
    "\n",
    "    def init_weights(self,x_dim):\n",
    "        np.random.seed(1)\n",
    "        self.layers.insert(0,[x_dim,'relu'])\n",
    "        \n",
    "        L = len(self.layers) -1 \n",
    "\n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(self.layers[l][0],self.layers[l-1][0]) * np.sqrt(2/self.layers[l-1][0])\n",
    "            self.parameters['b' + str(l)] = np.zeros((self.layers[l][0],1))\n",
    "\n",
    "\n",
    "\n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu_backward(self,dA,Z):\n",
    "        dZ = np.array(dA,copy=True)\n",
    "        dZ [Z <= 0 ] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def sigmoid_backward(self,dA,Z):\n",
    "        x = 1 / (1+np.exp(-Z))\n",
    "        return dA * x *(1-x)\n",
    "\n",
    "    def forward_propagation(self,X):\n",
    "        self.cache = []\n",
    "        L = len(self.parameters) // 2\n",
    "        \n",
    "        A = X\n",
    "\n",
    "        for l in range(1,L):\n",
    "            a_prev = A\n",
    "            W = self.parameters[f\"W{l}\"]\n",
    "            b = self.parameters[f\"b{l}\"]\n",
    "            Z = np.dot(W,a_prev) + b\n",
    "            \n",
    "            if self.layers[l][1] == 'relu':\n",
    "                A = self.relu(Z)\n",
    "            elif self.layers[l][1] == 'sigmoid':\n",
    "                A = self.sigmoid(Z)\n",
    "                \n",
    "            self.cache.append((a_prev,W,b,Z))\n",
    "\n",
    "        W = self.parameters[f\"W{L}\"]\n",
    "        b = self.parameters[f\"b{L}\"]\n",
    "        Z = np.dot(W,A) + b\n",
    "\n",
    "        if self.layers[L][1] == 'relu':\n",
    "            A = self.relu(Z)\n",
    "        elif self.layers[L][1] == 'sigmoid':\n",
    "            A = self.sigmoid(Z)\n",
    "            \n",
    "        self.cache.append((A,W,b,Z))\n",
    "        return A\n",
    "\n",
    "\n",
    "    def loss(self,Output,Y):\n",
    "        m = Y.shape[1]\n",
    "        if self.layers[-1][1] == 'relu':\n",
    "            cost = 1/m * np.sum(np.abs(Y-Output))\n",
    "            cost = np.squeeze(cost)\n",
    "        elif self.layers[-1][1] == 'sigmoid':\n",
    "            epsilon = 1e-8\n",
    "            cost = -1/m * np.sum(Y*np.log(Output+epsilon) + (1-Y) * np.log((1-Output)+epsilon))\n",
    "            cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def compute_grads(self,dZ,A,W):\n",
    "        m = A.shape[1]\n",
    "        dA = np.dot(W.T,dZ)\n",
    "        dW = 1/m * np.dot(dZ,A.T)\n",
    "        db = 1/m * np.sum(dZ,axis=1,keepdims=True)\n",
    "        return dA,dW,db\n",
    "\n",
    "    def backward_propagation(self,Y,A):\n",
    "        L = len(self.cache)\n",
    "\n",
    "        Y = Y.reshape(A.shape)\n",
    "        epsilon = 1e-8\n",
    "        #dA = - (np.divide(Y,A) - np.divide(1-Y,1-A))\n",
    "        dA = - (np.divide(Y, A + epsilon) - np.divide(1 - Y, 1 - A + epsilon))\n",
    "\n",
    "        A,W,b,Z = self.cache[L-1] \n",
    "        if self.layers[L-1][1] == 'relu':\n",
    "            dZ = self.relu_backward(dA,Z)\n",
    "        elif self.layers[L-1][1] == 'sigmoid':\n",
    "            dZ = self.sigmoid_backward(dA,Z)\n",
    "\n",
    "        dA,dW,db = self.compute_grads(dZ,A,W)\n",
    "        self.grads['dW' + str(L)] = dW\n",
    "        self.grads['db' + str(L)] = db\n",
    "        self.grads['dA' + str(L-1)] = dA\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            A,W,b,Z = self.cache[l]\n",
    "            if self.layers[l][1] == 'relu':\n",
    "                dZ = self.relu_backward(dA,Z)\n",
    "            elif self.layers[l][1] == 'sigmoid':\n",
    "                dZ = self.sigmoid_backward(dA,Z)\n",
    "    \n",
    "            dA,dW,db = self.compute_grads(dZ,A,W)\n",
    "            self.grads['dW' + str(l+1)] = dW\n",
    "            self.grads['db' + str(l+1)] = db\n",
    "            self.grads['dA' + str(l)] = dA\n",
    "\n",
    "\n",
    "    def update_parameters_gd(self,lr):\n",
    "        self.params = copy.deepcopy(self.parameters)\n",
    "        L = len(self.parameters) // 2\n",
    "    \n",
    "        for l in range(1,L+1):\n",
    "            self.params[f\"W{l}\"] = self.params[f\"W{l}\"] - lr * self.grads[f\"dW{l}\"]\n",
    "            self.params[f\"b{l}\"] = self.params[f\"b{l}\"] - lr * self.grads[f\"db{l}\"]\n",
    "\n",
    "        self.parameters = self.params\n",
    "\n",
    "\n",
    "    def lr_decay(self,lr,epoch,decay_rate=1,time_interval=1000):\n",
    "        learning_rate = (1 / (1+decay_rate * np.floor(epoch/time_interval))) * lr\n",
    "        return learning_rate\n",
    "\n",
    "    def update_parameters_adam(self,lr,t,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "        self.params = copy.deepcopy(self.parameters)\n",
    "        L = len(self.parameters) // 2\n",
    "\n",
    "        v_corrected = {}\n",
    "        s_corrected = {}\n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            self.v['dW'+str(l)] = beta1 * self.v['dW'+str(l)] + (1-beta1) * self.grads['dW'+str(l)]\n",
    "            self.v['db'+str(l)] = beta1 * self.v['db'+str(l)] + (1-beta1) * self.grads['db'+str(l)]\n",
    "            v_corrected['dW'+str(l)] = self.v['dW'+str(l)] / (1-beta1**t)\n",
    "            v_corrected['db'+str(l)] = self.v['db'+str(l)] / (1-beta1**t)\n",
    "            self.s['dW'+str(l)] = beta2 * self.s['dW'+str(l)] + (1-beta2) * self.grads['dW'+str(l)]**2\n",
    "            self.s['db'+str(l)] = beta2 * self.s['db'+str(l)] + (1-beta2) * self.grads['db'+str(l)]**2\n",
    "            s_corrected['dW'+str(l)] = self.s['dW'+str(l)] / (1-beta2**t)\n",
    "            s_corrected['db'+str(l)] = self.s['db'+str(l)] / (1-beta2**t)\n",
    "            self.params[f\"W{l}\"] = self.params[f\"W{l}\"] - lr * (v_corrected['dW'+str(l)]) / (np.sqrt(s_corrected['dW'+str(l)]) + epsilon) \n",
    "            self.params[f\"b{l}\"] = self.params[f\"b{l}\"] - lr * (v_corrected['db'+str(l)]) / (np.sqrt(s_corrected['db'+str(l)]) + epsilon) \n",
    "\n",
    "        self.parameters = self.params\n",
    "\n",
    "    def predict(self,X):\n",
    "        A = self.forward_propagation(X)\n",
    "        if self.layers[-1][1] == 'relu':\n",
    "            return A\n",
    "        elif self.layers[-1][1] == 'sigmoid':\n",
    "            pred = (A > 0.5).astype(int)\n",
    "            return pred\n",
    "\n",
    "\n",
    "    def compile(self,X):\n",
    "        x_dim = X.shape[0]\n",
    "        self.init_weights(x_dim)\n",
    "        self.initialize_adam()\n",
    "\n",
    "    def make_batches(self,batch_size,X,Y):\n",
    "        m = X.shape[1]\n",
    "        self.mini_batches = []\n",
    "        permutation = list(np.random.permutation(m))\n",
    "        shuffled_X = X[:, permutation]\n",
    "        shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "\n",
    "        n_batches = math.floor( m / batch_size)\n",
    "\n",
    "        for k in range(0,n_batches):\n",
    "            x_temp = shuffled_X[:,k*batch_size:(k+1)*batch_size]\n",
    "            y_temp = shuffled_Y[:,k*batch_size:(k+1)*batch_size]\n",
    "\n",
    "            temp = (x_temp,y_temp)\n",
    "            self.mini_batches.append(temp)\n",
    "\n",
    "\n",
    "        if m % batch_size != 0:\n",
    "            x_temp = shuffled_X[:,(k+1)*batch_size:]\n",
    "            y_temp = shuffled_Y[:,(k+1)*batch_size:]\n",
    "\n",
    "            temp = (x_temp,y_temp)\n",
    "            self.mini_batches.append(temp)\n",
    "\n",
    "    \n",
    "    def fit(self,X,Y,epochs=20,learning_rate=0.0001,batch_size=32,optimizer='adam'):\n",
    "        self.cache = []\n",
    "        self.grads = {}\n",
    "        self.params = {}\n",
    "        self.make_batches(batch_size,X,Y)\n",
    "\n",
    "        t = 0\n",
    "        \n",
    "        history = {}\n",
    "        history['loss'] = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            for batch in self.mini_batches:\n",
    "                (batch_X,batch_Y) = batch\n",
    "                A = self.forward_propagation(batch_X)\n",
    "                loss = self.loss(A,batch_Y)\n",
    "\n",
    "                self.backward_propagation(batch_Y,A)\n",
    "\n",
    "                if optimizer == 'gd':\n",
    "                    self.update_parameters_gd(learning_rate)\n",
    "                else:\n",
    "                    t = t + 1\n",
    "                    self.update_parameters_adam(learning_rate,t=t)\n",
    "\n",
    "\n",
    "            learning_rate = self.lr_decay(learning_rate,i)\n",
    "            \n",
    "            if i %10 ==0:\n",
    "                print(f\"Epochs : {i} , loss = {loss:.5f}\")\n",
    "                history['loss'].append(loss)\n",
    "                \n",
    "        return history\n",
    "\n",
    "\n",
    "    def initialize_adam(self):\n",
    "        self.v = {}\n",
    "        self.s = {}\n",
    "        L = len(self.parameters) // 2\n",
    "        \n",
    "        \n",
    "        for l in range(1,L+1):\n",
    "            self.v['dW'+str(l)] = np.zeros(self.parameters['W'+str(l)].shape)\n",
    "            self.v['db'+str(l)] = np.zeros(self.parameters['b'+str(l)].shape)\n",
    "            self.s['dW'+str(l)] = np.zeros(self.parameters['W'+str(l)].shape)\n",
    "            self.s['db'+str(l)] = np.zeros(self.parameters['b'+str(l)].shape)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e787fb3-c66c-486c-8626-69f2ff3776d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
