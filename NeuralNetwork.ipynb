{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712b3d41-a655-4979-85f3-61a6e4b9821a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing NeuralNetworkModel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile NeuralNetworkModel.py\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.parameters = {}\n",
    "        self.cache = []\n",
    "        self.grads = {}\n",
    "        self.params = {}\n",
    "    \n",
    "    def add(self,units,activation):\n",
    "        self.layers.append([units,activation])\n",
    "\n",
    "    def init_weights(self,x_dim):\n",
    "\n",
    "        self.layers.insert(0,[x_dim,'None'])\n",
    "        \n",
    "        L = len(self.layers) -1 \n",
    "\n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters['W' + str(l)] = np.random.randn(self.layers[l][0],self.layers[l-1][0]) * np.sqrt(2/self.layers[l-1][0])\n",
    "            self.parameters['b' + str(l)] = np.zeros((self.layers[l][0],1))\n",
    "\n",
    "\n",
    "\n",
    "    def relu(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def relu_backward(self,dA,Z):\n",
    "        dZ = np.array(dA,copy=True)\n",
    "        dZ [Z <= 0 ] = 0\n",
    "        return dZ\n",
    "    \n",
    "    def sigmoid_backward(self,dA,Z):\n",
    "        x = 1 / (1+np.exp(-Z))\n",
    "        return dA * x *(1-x)\n",
    "\n",
    "    def forward_propagation(self,X):\n",
    "\n",
    "        L = len(self.parameters) // 2\n",
    "        \n",
    "        A = X\n",
    "\n",
    "        for l in range(1,L):\n",
    "            a_prev = A\n",
    "            W = self.parameters[f\"W{l}\"]\n",
    "            b = self.parameters[f\"b{l}\"]\n",
    "            Z = np.dot(W,a_prev) + b\n",
    "            \n",
    "            if self.layers[l][1] == 'relu':\n",
    "                A = self.relu(Z)\n",
    "            elif self.layers[l][1] == 'sigmoid':\n",
    "                A = self.sigmoid(Z)\n",
    "                \n",
    "            self.cache.append((a_prev,W,b,Z))\n",
    "\n",
    "        W = self.parameters[f\"W{L}\"]\n",
    "        b = self.parameters[f\"b{L}\"]\n",
    "        Z = np.dot(W,A) + b\n",
    "        \n",
    "        if self.layers[L][1] == 'relu':\n",
    "            A = self.relu(Z)\n",
    "        elif self.layers[L][1] == 'sigmoid':\n",
    "            A = self.sigmoid(Z)\n",
    "            \n",
    "        self.cache.append((A,W,b,Z))\n",
    "        return A\n",
    "\n",
    "\n",
    "    def loss(self,Output,Y):\n",
    "        m = Y.shape[1]\n",
    "        if self.layers[-1][1] == 'relu':\n",
    "            cost = 1/m * np.sum((Y-Output)**2)\n",
    "        elif self.layers[-1][1] == 'sigmoid':\n",
    "            cost = -1/m * np.sum(Y*np.log(Output) + (1-Y) * np.log(1-Output))\n",
    "            cost = np.squeeze(cost)\n",
    "        return cost\n",
    "\n",
    "\n",
    "    def compute_grads(dZ,A,W):\n",
    "        m = A.shape[1]\n",
    "        dA = np.dot(W.T,dZ)\n",
    "        dW = 1/m * np.dot(dZ,A.T)\n",
    "        db = 1/m * np.sum(dZ,axis=1,keepdims=True)\n",
    "        return dA,dW,db\n",
    "\n",
    "    def backward_propagation(self,Y,A):\n",
    "\n",
    "        L = len(self.cache)\n",
    "\n",
    "        Y = Y.reshape(A.shape)\n",
    "        dA = - (np.divide(Y,A) - np.divide(1-Y,1-A))\n",
    "\n",
    "        A,W,b,Z = self.cache[L-1] \n",
    "        if self.layers[L][1] == 'relu':\n",
    "            dZ = self.relu_backward(dA,Z)\n",
    "        elif self.layers[L][1] == 'sigmoid':\n",
    "            dZ = self.sigmoid_backward(dA,Z)\n",
    "\n",
    "        dA,dW,dB = self.compute_grads(dZ,A,W)\n",
    "        self.grads['dW' + str(L)] = dW\n",
    "        self.grads['db' + str(L)] = db\n",
    "        self.grads['dA' + str(L-1)] = dA\n",
    "\n",
    "        for l in reversed(range(L-1)):\n",
    "            A,W,b,Z = self.cache[l]\n",
    "            if self.layers[l+1][1] == 'relu':\n",
    "                dZ = self.relu_backward(dA,Z)\n",
    "            elif self.layers[l+1][1] == 'sigmoid':\n",
    "                dZ = self.sigmoid_backward(dA,Z)\n",
    "    \n",
    "            dA,dW,dB = self.compute_grads(dZ,A,W)\n",
    "            self.grads['dW' + str(l+1)] = dW\n",
    "            self.grads['db' + str(l+1)] = db\n",
    "            self.grads['dA' + str(l)] = dA\n",
    "\n",
    "\n",
    "    def update_parameters(self,lr):\n",
    "        self.params = copy.deepcopy(self.parameters)\n",
    "        L = len(parameters) // 2\n",
    "    \n",
    "        for l in range(L):\n",
    "            self.params[f\"W{l+1}\"] = self.params[f\"W{l+1}\"] - lr * self.grads[f\"dW{l+1}\"]\n",
    "            self.params[f\"b{l+1}\"] = self.params[f\"b{l+1}\"] - lr * self.grads[f\"db{l+1}\"]\n",
    "\n",
    "\n",
    "    def predict(self,X):\n",
    "        A = self.forward_propagation(X)\n",
    "        if self.layers[-1][1] == 'relu':\n",
    "            return A\n",
    "        elif self.layers[-1][1] == 'sigmoid':\n",
    "            pred = (A > 0.5).astype(int)\n",
    "            return pred\n",
    "\n",
    "\n",
    "    def fit(self,X,Y,epochs=1000,learning_rate=0.001):\n",
    "        x_dim = X.shape[0]\n",
    "        self.init_weights(x_dim)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            A = self.forward_propagation(X)\n",
    "            loss = self.loss(A,Y)\n",
    "            self.backward_propagation(Y,A)\n",
    "            self.update_parameters(learning_rate)\n",
    "\n",
    "            if i %100 ==0:\n",
    "                print(f\"Epochs : {i} , loss = {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5641e-8bb4-4d8b-aa7b-a4f8d430ce8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
